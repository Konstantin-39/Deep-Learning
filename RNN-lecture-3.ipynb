{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Загрузим и сделаем предварительную обработку данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-YlRH3mQM9tf"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MIEGXF8oM9tt"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "colab_type": "code",
        "id": "8UKlPFcBNZl5",
        "outputId": "c4eb79b7-0097-427e-f25c-a5f5e9473449"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2025-02-08 19:09:55--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.239.83.32, 18.239.83.16, 18.239.83.126, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.239.83.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2,7M) [application/zip]\n",
            "Saving to: 'data.zip'\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  1%  515K 5s\n",
            "    50K .......... .......... .......... .......... ..........  3%  550K 5s\n",
            "   100K .......... .......... .......... .......... ..........  5% 1,91M 4s\n",
            "   150K .......... .......... .......... .......... ..........  7%  857K 4s\n",
            "   200K .......... .......... .......... .......... ..........  8% 6,28M 3s\n",
            "   250K .......... .......... .......... .......... .......... 10% 1,21M 3s\n",
            "   300K .......... .......... .......... .......... .......... 12% 29,9M 2s\n",
            "   350K .......... .......... .......... .......... .......... 14% 1,00M 2s\n",
            "   400K .......... .......... .......... .......... .......... 15% 37,0M 2s\n",
            "   450K .......... .......... .......... .......... .......... 17% 35,0M 2s\n",
            "   500K .......... .......... .......... .......... .......... 19% 7,49M 2s\n",
            "   550K .......... .......... .......... .......... .......... 21% 2,89M 1s\n",
            "   600K .......... .......... .......... .......... .......... 23% 9,68M 1s\n",
            "   650K .......... .......... .......... .......... .......... 24% 9,45M 1s\n",
            "   700K .......... .......... .......... .......... .......... 26% 10,2M 1s\n",
            "   750K .......... .......... .......... .......... .......... 28%  614K 1s\n",
            "   800K .......... .......... .......... .......... .......... 30% 61,9M 1s\n",
            "   850K .......... .......... .......... .......... .......... 31% 21,1M 1s\n",
            "   900K .......... .......... .......... .......... .......... 33% 54,9M 1s\n",
            "   950K .......... .......... .......... .......... .......... 35% 64,9M 1s\n",
            "  1000K .......... .......... .......... .......... .......... 37% 59,2M 1s\n",
            "  1050K .......... .......... .......... .......... .......... 39% 58,4M 1s\n",
            "  1100K .......... .......... .......... .......... .......... 40% 59,1M 1s\n",
            "  1150K .......... .......... .......... .......... .......... 42% 64,4M 1s\n",
            "  1200K .......... .......... .......... .......... .......... 44% 66,3M 1s\n",
            "  1250K .......... .......... .......... .......... .......... 46% 1,70M 1s\n",
            "  1300K .......... .......... .......... .......... .......... 47% 1,03M 1s\n",
            "  1350K .......... .......... .......... .......... .......... 49% 64,5M 1s\n",
            "  1400K .......... .......... .......... .......... .......... 51% 6,15M 1s\n",
            "  1450K .......... .......... .......... .......... .......... 53% 64,1M 1s\n",
            "  1500K .......... .......... .......... .......... .......... 55% 63,4M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 56% 57,6M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 58% 57,5M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 60% 66,7M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 62% 47,7M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 63%  504K 0s\n",
            "  1800K .......... .......... .......... .......... .......... 65% 12,1M 0s\n",
            "  1850K .......... .......... .......... .......... .......... 67% 51,4M 0s\n",
            "  1900K .......... .......... .......... .......... .......... 69% 62,4M 0s\n",
            "  1950K .......... .......... .......... .......... .......... 71% 69,5M 0s\n",
            "  2000K .......... .......... .......... .......... .......... 72% 61,3M 0s\n",
            "  2050K .......... .......... .......... .......... .......... 74% 54,5M 0s\n",
            "  2100K .......... .......... .......... .......... .......... 76% 68,1M 0s\n",
            "  2150K .......... .......... .......... .......... .......... 78% 52,6M 0s\n",
            "  2200K .......... .......... .......... .......... .......... 79% 52,3M 0s\n",
            "  2250K .......... .......... .......... .......... .......... 81% 58,6M 0s\n",
            "  2300K .......... .......... .......... .......... .......... 83% 61,1M 0s\n",
            "  2350K .......... .......... .......... .......... .......... 85% 62,2M 0s\n",
            "  2400K .......... .......... .......... .......... .......... 87% 49,0M 0s\n",
            "  2450K .......... .......... .......... .......... .......... 88% 69,2M 0s\n",
            "  2500K .......... .......... .......... .......... .......... 90% 28,0M 0s\n",
            "  2550K .......... .......... .......... .......... .......... 92%  976K 0s\n",
            "  2600K .......... .......... .......... .......... .......... 94% 11,3M 0s\n",
            "  2650K .......... .......... .......... .......... .......... 95% 19,1M 0s\n",
            "  2700K .......... .......... .......... .......... .......... 97% 60,1M 0s\n",
            "  2750K .......... .......... .......... .......... .......... 99% 49,7M 0s\n",
            "  2800K .......... ....                                       100%  184M=0,8s\n",
            "\n",
            "2025-02-08 19:09:56 (3,58 MB/s) - 'data.zip' saved [2882130/2882130]\n",
            "\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"D:\\Progect\\RNN\\RNN\\data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"D:\\Progect\\RNN\\RNN\")       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "colab_type": "code",
        "id": "twIcAJnyRkW-",
        "outputId": "aae61acf-df6c-4443-8eaa-61a0be531bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Five tremors in excess of magnitude 5.0 on the Richter scale have shaken Japan just this week, but scientists are warning that the largest expected aftershock has yet to hit.\tCinq secousses dépassant la magnitude cinq sur l'échelle de Richter ont secoué le Japon précisément cette semaine, mais les scientifiques avertissent que la plus grande réplique est encore à venir.\n",
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\tPeu importe le temps que tu passeras à essayer de convaincre les gens que le chocolat est de la vanille, ça restera toujours du chocolat, même si tu réussis à convaincre toi et quelques autres que c'est de la vanille.\n",
            "A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.\tUn enfant qui est un locuteur natif connaît habituellement de nombreuses choses sur son langage qu'un locuteur non-natif qui a étudié pendant des années ignore encore et peut-être ne saura jamais.\n",
            "There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.\tIl y a quatre causes principales de décès liés à l'alcool. Les blessures dans les accidents automobiles ou la violence en est une. Les maladies comme la cirrhose, le cancer, les maladies cardio-vasculaires en sont les autres.\n",
            "\"Top-down economics never works,\" said Obama. \"The country does not succeed when just those at the very top are doing well. We succeed when the middle class gets bigger, when it feels greater security.\"\t« L'économie en partant du haut vers le bas, ça ne marche jamais, » a dit Obama. « Le pays ne réussit pas lorsque seulement ceux qui sont au sommet s'en sortent bien. Nous réussissons lorsque la classe moyenne s'élargit, lorsqu'elle se sent davantage en sécurité. »\n",
            "A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.\tUne empreinte carbone est la somme de pollution au dioxyde de carbone que nous produisons par nos activités. Certaines personnes essaient de réduire leur empreinte carbone parce qu'elles sont inquiètes du changement climatique.\n",
            "Death is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\tLa mort est une chose qu'on nous décourage souvent de discuter ou même de penser mais j'ai pris conscience que se préparer à la mort est l'une des choses que nous puissions faire qui nous investit le plus de responsabilité. Réfléchir à la mort clarifie notre vie.\n",
            "Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tPuisqu'il y a de multiples sites web sur chaque sujet, je clique d'habitude sur le bouton retour arrière lorsque j'atterris sur n'importe quelle page qui contient des publicités surgissantes. Je me rends juste sur la prochaine page proposée par Google et espère tomber sur quelque chose de moins irritant.\n",
            "If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\tSi quelqu'un qui ne connaît pas vos antécédents dit que vous parlez comme un locuteur natif, cela veut dire qu'il a probablement remarqué quelque chose à propos de votre élocution qui l'a fait prendre conscience que vous n'êtes pas un locuteur natif. En d'autres termes, vous ne parlez pas vraiment comme un locuteur natif.\n",
            "It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.\tIl est peut-être impossible d'obtenir un Corpus complètement dénué de fautes, étant donnée la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres à produire des phrases dans leurs propres langues plutôt que d'expérimenter dans les langues qu'ils apprennent, nous pourrions être en mesure de réduire les erreurs.\n"
          ]
        }
      ],
      "source": [
        "with open(r\"D:\\Progect\\RNN\\RNN\\data\\eng-fra.txt\", 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines[-10:]:  # Shows last 10 lines\n",
        "        print(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kyNnJyruM9t1"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0   # Start of sequence\n",
        "EOS_token = 1   # End of sequence\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FXKs8j4bM9t6"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "D8T4VxZeM9t-"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eBOwgEBdM9uB"
      },
      "outputs": [],
      "source": [
        "# максимальное количество слов в предложении\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "ENGLISH_PREFIXES = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filter_pair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(ENGLISH_PREFIXES)\n",
        "\n",
        "def filter_pairs(pairs):\n",
        "    return [pair for pair in pairs if filter_pair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "colab_type": "code",
        "id": "6dZOGjd5M9uE",
        "outputId": "0cdd3a7f-2ac8-4872-8a81-6101d0bdd0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10853 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4489\n",
            "eng 2925\n",
            "['vous m intimidez toujours .', 'i m still intimidated by you .']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vgtWqznCM9uH"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "m9vm9QBWM9uI"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwLTlgSyM9uK"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PFbuUL1LM9uL"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "z6gGPtXFM9uQ"
      },
      "outputs": [],
      "source": [
        "# преобразуем предложения в список индексов и слов\n",
        "def indexes_from_sentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "# преобразуем предложение в тензор PyTorch\n",
        "def tensor_from_sentence(lang, sentence):\n",
        "    indexes = indexes_from_sentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# создадим тензоры для пары предложений\n",
        "def tensors_from_pair(pair):\n",
        "    input_tensor = tensor_from_sentence(input_lang, pair[0])\n",
        "    target_tensor = tensor_from_sentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8Fn8VDv8M9uS"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# обучение модели\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "     # Инициализация\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Кодирование входного предложения\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    # декодирование\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Обучение с teacher forcing или без него\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "     # Обратное распространение ошибки и обновление моделей\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EKsdwPmSM9uU"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def as_minutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0JXG-RzCM9uZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def show_plot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C_z_k5IiM9uX"
      },
      "outputs": [],
      "source": [
        "# обучение модели\n",
        "def train_iterations(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    # Инициализация оптимизаторов\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Подготовка данных для обучения\n",
        "    training_pairs = [tensors_from_pair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # Цикл обучения\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        # Получение пары для обучения\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        # Обучение на текущей паре\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        \n",
        "        # Накопление потерь\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "         # Печать статистики\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (time_since(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        # Обновление графика\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    show_plot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3Bxf45h6M9ud"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # Подготовка входных данных\n",
        "        input_tensor = tensor_from_sentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        # Инициализация кодировщика\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        # Кодирование входного предложения\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        # Инициализация декодировщика\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "\n",
        "        # Генерация перевода\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1qUmQIGwM9uf"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "colab_type": "code",
        "id": "s_56t10oM9uh",
        "outputId": "f456b0b8-fc35-4199-fb19-b45c2330bf72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3m 17s (- 46m 10s) (5000 6%) 2.9879\n",
            "6m 37s (- 43m 1s) (10000 13%) 2.4044\n",
            "9m 47s (- 39m 11s) (15000 20%) 2.0944\n",
            "13m 14s (- 36m 24s) (20000 26%) 1.8410\n",
            "16m 27s (- 32m 54s) (25000 33%) 1.6401\n",
            "19m 49s (- 29m 43s) (30000 40%) 1.4658\n",
            "23m 11s (- 26m 30s) (35000 46%) 1.3079\n",
            "26m 37s (- 23m 18s) (40000 53%) 1.1770\n",
            "30m 2s (- 20m 1s) (45000 60%) 1.0392\n",
            "33m 30s (- 16m 45s) (50000 66%) 0.9670\n",
            "36m 53s (- 13m 24s) (55000 73%) 0.8399\n",
            "40m 20s (- 10m 5s) (60000 80%) 0.7524\n",
            "43m 44s (- 6m 43s) (65000 86%) 0.6624\n",
            "47m 8s (- 3m 22s) (70000 93%) 0.6139\n",
            "50m 31s (- 0m 0s) (75000 100%) 0.5693\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train_iterations(encoder1, decoder1, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xEoEylSyM9uj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> je m en sors .\n",
            "= i m managing .\n",
            "< i m managing . <EOS>\n",
            "\n",
            "> vous etes plus intelligent que moi .\n",
            "= you re smarter than me .\n",
            "< you re smarter than me . <EOS>\n",
            "\n",
            "> je suis a bout de souffle .\n",
            "= i m short of breath .\n",
            "< i m familiar of of . <EOS>\n",
            "\n",
            "> elles sont toutes mauvaises .\n",
            "= they re all bad .\n",
            "< they re all bad . <EOS>\n",
            "\n",
            "> j en ai assez de me disputer .\n",
            "= i m tired of arguing .\n",
            "< i m tired of arguing . <EOS>\n",
            "\n",
            "> je suis vraiment desole pour l erreur .\n",
            "= i m very sorry about the mistake .\n",
            "< i m sorry for my mistake . <EOS>\n",
            "\n",
            "> ils vont faire des conneries .\n",
            "= they re up to no good .\n",
            "< they re up to no good . <EOS>\n",
            "\n",
            "> je suis de la cote est .\n",
            "= i m from the east coast .\n",
            "< i m from the mood . <EOS>\n",
            "\n",
            "> je m ennuie a en mourir .\n",
            "= i am bored to death .\n",
            "< i am bored to death . <EOS>\n",
            "\n",
            "> je ne suis pas tres organise .\n",
            "= i m not very organized .\n",
            "< i m not very . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Лекция 8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
