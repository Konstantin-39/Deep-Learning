{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Задание 1**   \n",
    "Обучим нейронную сеть решать шифр Цезаря\n",
    "\n",
    "*Что необходимо сделать:*\n",
    "\n",
    "1. Написать алгоритм шифра Цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)\n",
    "2. Сделать нейронную сеть\n",
    "3. Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
    "4. Проверить качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Задание 2**   \n",
    "Выполнить практическую работу из лекционного ноутбука\n",
    "\n",
    "1. Построить RNN-ячейку на основе полносвязных слоев\n",
    "2. Применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Написать алгоритм шифра Цезаря\n",
    "\n",
    "\n",
    "> Шифр Цезаря - это простой тип подстановочного шифра, где каждая буква текста заменяется буквой с фиксированным числом позиций вниз по алфавиту.   \n",
    "> Создадим слудующие переменные:  \n",
    "> alphabets - словарь с алфавитом \n",
    "> K - для задания шага сдвига букв, которая будет вручную задаваться пользователем.   \n",
    "> text - исходный текст\n",
    "> encrypted_text - куда мы будем выводить зашифрованное сообщение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим словарь\n",
    "# Ключи 'ru' и 'en' используются для выбора соответствующего алфавита\n",
    "alphabets = {\n",
    "    'ru': 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя',\n",
    "    'en': 'abcdefghijklmnopqrstuvwxyz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для шифра Цезаря\n",
    "def caesar_cipher(text, K, alphabet):\n",
    "    result = \" \"\n",
    "    for char in text:\n",
    "            if char.lower() in alphabet:\n",
    "                is_upper = char.isupper()\n",
    "                char = char.lower()\n",
    "                index = (alphabet.find(char) + K ) % len(alphabet)\n",
    "                shifted_char = alphabet[index].upper() if is_upper else alphabet[index]\n",
    "                result += shifted_char\n",
    "            else:\n",
    "                result += char\n",
    "    return result       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Исходный текст: привет Мир\n",
      "Шаг шифрования: 10\n",
      "Выбранный алфавит: ru\n",
      "Зашифрованный текст:  щътлоь Цтъ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ввод данных\n",
    "text = input('введите текст: ')\n",
    "K = int(input('Шаг шифровки: '))\n",
    "alphabet_choice = input('Выберите алфавит (ru/en): '). strip().lower()\n",
    "\n",
    "# Выбор алфавита (по умолчанию русский)\n",
    "alphabet = alphabets.get(alphabet_choice, alphabets['ru'])\n",
    "\n",
    "# Шифрование и вывод результата\n",
    "encrypted_text = caesar_cipher(text, K, alphabet)\n",
    "\n",
    "print(f\"\"\"\n",
    "Исходный текст: {text}\n",
    "Шаг шифрования: {K}\n",
    "Выбранный алфавит: {alphabet_choice}\n",
    "Зашифрованный текст: {encrypted_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Создадим нейронную сеть   \n",
    "\n",
    "Для решения задачи подойдет рекуррентная нейронная сеть (RNN) или её улучшенная версия — LSTM (Long Short-Term Memory). Эти архитектуры хорошо работают с последовательностями, такими как текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Константы\n",
    "CHARS = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "INDEX_TO_CHAR = ['none'] + list(CHARS)\n",
    "CHAR_TO_INDEX = {char: idx for idx, char in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шифр Цезаря\n",
    "def caesar_cipher(text, shift):\n",
    "    return ''.join([INDEX_TO_CHAR[(CHAR_TO_INDEX.get(char, 0) - 1 + shift) % len(CHARS) + 1] \n",
    "                    if char in CHARS else char for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейронная сеть\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(INDEX_TO_CHAR), 30)\n",
    "        self.rnn = nn.RNN(30, 128, batch_first=True)\n",
    "        self.out = nn.Linear(128, len(INDEX_TO_CHAR))\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        x = self.embedding(sentences)\n",
    "        x, _ = self.rnn(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "def generate_dataset(texts, shifts):\n",
    "    return [(caesar_cipher(text, random.choice(shifts)), text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование текста в тензор\n",
    "def text_to_tensor(texts):\n",
    "    max_len = max(len(text) for text in texts)\n",
    "    tensor = torch.zeros((len(texts), max_len), dtype=torch.long)\n",
    "    for i, text in enumerate(texts):\n",
    "        for j, char in enumerate(text):\n",
    "            tensor[i, j] = CHAR_TO_INDEX.get(char, 0)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дешифровка текста\n",
    "def decrypt_text(model, encrypted_text):\n",
    "    encrypted_tensor = text_to_tensor([encrypted_text])\n",
    "    output = model(encrypted_tensor)\n",
    "    _, indices = output.topk(1)\n",
    "    return ''.join([INDEX_TO_CHAR[idx.item()] for idx in indices.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "def train_model(model, X, Y, epochs=50, lr=0.05):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = 0.0, 0.0\n",
    "        for i in range(len(X)):\n",
    "            optimizer.zero_grad()\n",
    "            X_tensor = text_to_tensor([X[i]])\n",
    "            Y_tensor = text_to_tensor([Y[i]])\n",
    "            output = model(X_tensor).view(-1, len(INDEX_TO_CHAR))\n",
    "            target = Y_tensor.view(-1)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_accuracy += (predicted == target).sum().item() / len(target)\n",
    "        \n",
    "        print(f\"Epoch {epoch}. Loss: {train_loss / len(X):.3f}, Accuracy: {train_accuracy / len(X):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование модели\n",
    "def test_model(model, original_text, shift):\n",
    "    encrypted_text = caesar_cipher(original_text, shift)\n",
    "    decrypted_text = decrypt_text(model, encrypted_text)\n",
    "    print(f\"Original: {original_text}\\nEncrypted: {encrypted_text}\\nDecrypted: {decrypted_text}\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "texts = [\n",
    "    \"hello world\", \"neural network\", \"caesar cipher\", \"deep learning\",\n",
    "    \"machine learning\", \"artificial intelligence\", \"data science\",\n",
    "    \"python programming\", \"pytorch framework\", \"encryption algorithms\"\n",
    "]\n",
    "shifts = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3.333, Accuracy: 0.066\n",
      "Epoch 1. Loss: 3.147, Accuracy: 0.229\n",
      "Epoch 2. Loss: 2.961, Accuracy: 0.354\n",
      "Epoch 3. Loss: 2.771, Accuracy: 0.411\n",
      "Epoch 4. Loss: 2.583, Accuracy: 0.425\n",
      "Epoch 5. Loss: 2.413, Accuracy: 0.407\n",
      "Epoch 6. Loss: 2.267, Accuracy: 0.440\n",
      "Epoch 7. Loss: 2.138, Accuracy: 0.460\n",
      "Epoch 8. Loss: 2.019, Accuracy: 0.488\n",
      "Epoch 9. Loss: 1.909, Accuracy: 0.511\n",
      "Epoch 10. Loss: 1.807, Accuracy: 0.523\n",
      "Epoch 11. Loss: 1.710, Accuracy: 0.541\n",
      "Epoch 12. Loss: 1.620, Accuracy: 0.587\n",
      "Epoch 13. Loss: 1.534, Accuracy: 0.608\n",
      "Epoch 14. Loss: 1.453, Accuracy: 0.612\n",
      "Epoch 15. Loss: 1.376, Accuracy: 0.641\n",
      "Epoch 16. Loss: 1.302, Accuracy: 0.657\n",
      "Epoch 17. Loss: 1.232, Accuracy: 0.676\n",
      "Epoch 18. Loss: 1.165, Accuracy: 0.705\n",
      "Epoch 19. Loss: 1.101, Accuracy: 0.734\n",
      "Epoch 20. Loss: 1.040, Accuracy: 0.739\n",
      "Epoch 21. Loss: 0.982, Accuracy: 0.739\n",
      "Epoch 22. Loss: 0.926, Accuracy: 0.758\n",
      "Epoch 23. Loss: 0.873, Accuracy: 0.768\n",
      "Epoch 24. Loss: 0.822, Accuracy: 0.793\n",
      "Epoch 25. Loss: 0.773, Accuracy: 0.806\n",
      "Epoch 26. Loss: 0.727, Accuracy: 0.833\n",
      "Epoch 27. Loss: 0.684, Accuracy: 0.838\n",
      "Epoch 28. Loss: 0.643, Accuracy: 0.864\n",
      "Epoch 29. Loss: 0.604, Accuracy: 0.871\n",
      "Epoch 30. Loss: 0.568, Accuracy: 0.871\n",
      "Epoch 31. Loss: 0.533, Accuracy: 0.888\n",
      "Epoch 32. Loss: 0.502, Accuracy: 0.903\n",
      "Epoch 33. Loss: 0.472, Accuracy: 0.903\n",
      "Epoch 34. Loss: 0.444, Accuracy: 0.918\n",
      "Epoch 35. Loss: 0.418, Accuracy: 0.924\n",
      "Epoch 36. Loss: 0.394, Accuracy: 0.937\n",
      "Epoch 37. Loss: 0.372, Accuracy: 0.959\n",
      "Epoch 38. Loss: 0.351, Accuracy: 0.969\n",
      "Epoch 39. Loss: 0.332, Accuracy: 0.975\n",
      "Epoch 40. Loss: 0.314, Accuracy: 0.981\n",
      "Epoch 41. Loss: 0.297, Accuracy: 0.985\n",
      "Epoch 42. Loss: 0.282, Accuracy: 0.985\n",
      "Epoch 43. Loss: 0.267, Accuracy: 0.991\n",
      "Epoch 44. Loss: 0.254, Accuracy: 0.991\n",
      "Epoch 45. Loss: 0.242, Accuracy: 0.996\n",
      "Epoch 46. Loss: 0.230, Accuracy: 0.996\n",
      "Epoch 47. Loss: 0.219, Accuracy: 0.996\n",
      "Epoch 48. Loss: 0.209, Accuracy: 0.996\n",
      "Epoch 49. Loss: 0.200, Accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "X, Y = zip(*generate_dataset(texts, shifts))\n",
    "model = Network()\n",
    "train_model(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: hello world\n",
      "Encrypted: nyiihebhviq\n",
      "Decrypted: a glhminrka\n",
      "----------------------------------------\n",
      "Original: hello world\n",
      "Encrypted: mkccnysnwcf\n",
      "Decrypted: oella warli\n",
      "----------------------------------------\n",
      "Original: hello world\n",
      "Encrypted: ptrrmkdmbra\n",
      "Decrypted: hello world\n",
      "----------------------------------------\n",
      "Original: hello world\n",
      "Encrypted: jzvvptqpsvx\n",
      "Decrypted: p lcheah ia\n",
      "----------------------------------------\n",
      "Original: hello world\n",
      "Encrypted:  gwwjzfjdwu\n",
      "Decrypted: mtcrptlprra\n",
      "----------------------------------------\n",
      "Original: neural network\n",
      "Encrypted: myovxiemyzbhvt\n",
      "Decrypted: omacsiniewlorc\n",
      "----------------------------------------\n",
      "Original: neural network\n",
      "Encrypted: pkhwucypkgsnwz\n",
      "Decrypted: hencal oetchr \n",
      "----------------------------------------\n",
      "Original: neural network\n",
      "Encrypted: jtnborkjtldmbg\n",
      "Decrypted: pearai peraore\n",
      "----------------------------------------\n",
      "Original: neural network\n",
      "Encrypted:  zmshvt ziqpsl\n",
      "Decrypted: m orarep lrnca\n",
      "----------------------------------------\n",
      "Original: neural network\n",
      "Encrypted: egpdnwzegcfjdi\n",
      "Decrypted: neural network\n",
      "----------------------------------------\n",
      "Original: caesar cipher\n",
      "Encrypted: rxydxvercjnyv\n",
      "Decrypted: ifprrinitho i\n",
      "----------------------------------------\n",
      "Original: caesar cipher\n",
      "Encrypted: vukquwyvr mkw\n",
      "Decrypted: cdealrpilmoec\n",
      "----------------------------------------\n",
      "Original: caesar cipher\n",
      "Encrypted: wotfobkwveptb\n",
      "Decrypted: caesar cipher\n",
      "----------------------------------------\n",
      "Original: caesar cipher\n",
      "Encrypted: bhzahstbwyjzs\n",
      "Decrypted: rorlarerlpptn\n",
      "----------------------------------------\n",
      "Original: caesar cipher\n",
      "Encrypted: sngxndzsbk gd\n",
      "Decrypted: caesor ciencr\n",
      "----------------------------------------\n",
      "Original: deep learning\n",
      "Encrypted: qyyjeiyxvmcml\n",
      "Decrypted: apppngnwcomwr\n",
      "----------------------------------------\n",
      "Original: deep learning\n",
      "Encrypted: fkk yckuwprpi\n",
      "Decrypted: seenplearhing\n",
      "----------------------------------------\n",
      "Original: deep learning\n",
      "Encrypted: attekrtobjvjc\n",
      "Decrypted: deenelearning\n",
      "----------------------------------------\n",
      "Original: deep learning\n",
      "Encrypted: xzzytvzhs w r\n",
      "Decrypted: f  eylro ncng\n",
      "----------------------------------------\n",
      "Original: deep learning\n",
      "Encrypted: uggkzwgndebev\n",
      "Decrypted: deep learning\n",
      "----------------------------------------\n",
      "Original: machine learning\n",
      "Encrypted: pxrncmyeiyxvmcml\n",
      "Decrypted: hflotomngngcomar\n",
      "----------------------------------------\n",
      "Original: machine learning\n",
      "Encrypted: juvmrpkyckuwprpi\n",
      "Decrypted: pdrognenl arhing\n",
      "----------------------------------------\n",
      "Original: machine learning\n",
      "Encrypted:  owpvjtkrtobjvjc\n",
      "Decrypted: machine learning\n",
      "----------------------------------------\n",
      "Original: machine learning\n",
      "Encrypted: ehbjw ztvzhs w r\n",
      "Decrypted: narncn ecro ncng\n",
      "----------------------------------------\n",
      "Original: machine learning\n",
      "Encrypted: yns begzwgndebev\n",
      "Decrypted: patnrnetlearning\n",
      "----------------------------------------\n",
      "Original: artificial intelligence\n",
      "Encrypted: xvzcacrcxiecmzyiiclymry\n",
      "Decrypted: fcetdlglwcngo  gllwniln\n",
      "----------------------------------------\n",
      "Original: artificial intelligence\n",
      "Encrypted: uwgrxrvrucyrpgkccrikpvk\n",
      "Decrypted: drtificial intelligence\n",
      "----------------------------------------\n",
      "Original: artificial intelligence\n",
      "Encrypted: oblvuvwvorkvjltrrvctjwt\n",
      "Decrypted: aracdrciat iprelllt nre\n",
      "----------------------------------------\n",
      "Original: artificial intelligence\n",
      "Encrypted: hsiwowbwhvtw izvvwrz bz\n",
      "Decrypted: orilacschgelpg icri nr \n",
      "----------------------------------------\n",
      "Original: artificial intelligence\n",
      "Encrypted: ndcbhbsbnwzbecgwwbvgesg\n",
      "Decrypted: arliorcior ingecriiepce\n",
      "----------------------------------------\n",
      "Original: data science\n",
      "Encrypted: qxzxedrcymry\n",
      "Decrypted: as iniklpoin\n",
      "----------------------------------------\n",
      "Original: data science\n",
      "Encrypted: fuguyqvrkpvk\n",
      "Decrypted: sdeapali nce\n",
      "----------------------------------------\n",
      "Original: data science\n",
      "Encrypted: aolokfwvtjwt\n",
      "Decrypted: daraesiiepce\n",
      "----------------------------------------\n",
      "Original: data science\n",
      "Encrypted: xhihtabwz bz\n",
      "Decrypted: fognelil nr \n",
      "----------------------------------------\n",
      "Original: data science\n",
      "Encrypted: uncnzxsbgesg\n",
      "Decrypted: data science\n",
      "----------------------------------------\n",
      "Original: python programming\n",
      "Encrypted: jkznhmejvhlvxppcml\n",
      "Decrypted: pyto omminrcimetar\n",
      "----------------------------------------\n",
      "Original: python programming\n",
      "Encrypted:  tgmnpy wniwujjrpi\n",
      "Decrypted: meehon aragnapping\n",
      "----------------------------------------\n",
      "Original: python programming\n",
      "Encrypted: ezlpmjkebmcbo  vjc\n",
      "Decrypted: n nnomyniogwammcpg\n",
      "----------------------------------------\n",
      "Original: python programming\n",
      "Encrypted: ygijp tysprsheew r\n",
      "Decrypted: python programming\n",
      "----------------------------------------\n",
      "Original: python programming\n",
      "Encrypted: klc jezkdjvdnyybev\n",
      "Decrypted: ewlmnn ewoira ping\n",
      "----------------------------------------\n",
      "Original: pytorch framework\n",
      "Encrypted: jkzhvrneavxpybhvt\n",
      "Decrypted: pytorch framework\n",
      "----------------------------------------\n",
      "Original: pytorch framework\n",
      "Encrypted:  tgnwvmyxwujksnwz\n",
      "Decrypted: meeario scapycor \n",
      "----------------------------------------\n",
      "Original: pytorch framework\n",
      "Encrypted: ezlmbwpkubo tdmbg\n",
      "Decrypted: n norlhearam aore\n",
      "----------------------------------------\n",
      "Original: pytorch framework\n",
      "Encrypted: ygipsbjtoshezqpsl\n",
      "Decrypted: pythcinearom ahra\n",
      "----------------------------------------\n",
      "Original: pytorch framework\n",
      "Encrypted: klcjds zhdnygfjdi\n",
      "Decrypted: ewl ara orapethrk\n",
      "----------------------------------------\n",
      "Original: encryption algorithms\n",
      "Encrypted: ymrvkjzchmexilhvcznpd\n",
      "Decrypted: pglrepilaomichorl onr\n",
      "----------------------------------------\n",
      "Original: encryption algorithms\n",
      "Encrypted: kpvwt grnpyucinwrgmjq\n",
      "Decrypted: encryption algorithms\n",
      "----------------------------------------\n",
      "Original: encryption algorithms\n",
      "Encrypted: tjwbzelvmjkorcmbvlp f\n",
      "Decrypted: epli nachmyaetornahmw\n",
      "----------------------------------------\n",
      "Original: encryption algorithms\n",
      "Encrypted: z bsgyiwp thvrpswijea\n",
      "Decrypted:  nrcepglhm orch lipmi\n",
      "----------------------------------------\n",
      "Original: encryption algorithms\n",
      "Encrypted: gesdlkcbjeznwvjdbc yx\n",
      "Decrypted: epcwr linn ariprrlm w\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    for shift in shifts:\n",
    "        test_model(model, text, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 1: Построение RNN-ячейки на основе полносвязных слоев\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Полносвязные слои\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Вход + скрытое состояние -> скрытое состояние\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)  # Вход + скрытое состояние -> выход\n",
    "        self.activation = nn.Tanh()  # Активация\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Объединяем вход и скрытое состояние\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        # Вычисляем новое скрытое состояние\n",
    "        hidden = self.activation(self.i2h(combined))\n",
    "        # Вычисляем выход\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Инициализация скрытого состояния\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 2: Подготовка данных\n",
    "# Загрузка данных (пример)\n",
    "text = ('simpsons_script_lines')\n",
    "\n",
    "# Создание словаря символов\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {char: idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
    "\n",
    "# Преобразование текста в индексы\n",
    "text_as_int = [char_to_index[char] for char in text]\n",
    "\n",
    "# Создание последовательностей для обучения\n",
    "seq_length = 100  # Длина последовательности\n",
    "examples = [text_as_int[i:i + seq_length + 1] for i in range(len(text_as_int) - seq_length)]\n",
    "\n",
    "# Разделение на вход и цель\n",
    "inputs = [example[:-1] for example in examples]\n",
    "targets = [example[1:] for example in examples]\n",
    "\n",
    "# Преобразование в тензоры\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 3: Создание модели\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = CustomRNNCell(input_size, hidden_size, output_size)\n",
    "        self.embedding = nn.Embedding(output_size, input_size)  # Эмбеддинг для символов\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Применяем эмбеддинг\n",
    "        embedded = self.embedding(input)\n",
    "        # Проходим через RNN\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.rnn.init_hidden(batch_size)\n",
    "\n",
    "# Параметры модели\n",
    "input_size = 128  # Размерность эмбеддинга\n",
    "hidden_size = 256  # Размер скрытого состояния\n",
    "output_size = len(chars)  # Размер словаря\n",
    "\n",
    "# Создание модели\n",
    "model = RNNModel(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Вывод средней потери за эпоху\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Шаг 4: Обучение модели\n",
    "# Параметры обучения\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Обучение\n",
    "for epoch in range(epochs):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    total_loss = 0  # Для накопления потерь за эпоху\n",
    "\n",
    "    for i in range(0, inputs.size(0), batch_size):\n",
    "        # Получаем батч\n",
    "        input_batch = inputs[i:i + batch_size]\n",
    "        target_batch = targets[i:i + batch_size]\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output, hidden = model(input_batch, hidden)\n",
    "        loss = criterion(output.view(-1, output_size), target_batch.view(-1))\n",
    "\n",
    "        # Обратный проход и обновление весов\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Отсоединяем скрытое состояние для следующего батча\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        # Накопление потерь\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Вывод средней потери за эпоху\n",
    "    avg_loss = total_loss / (inputs.size(0) // batch_size)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'H'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m start_string \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_generated)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Пример генерации текста\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHomer: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[54], line 4\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, start_string, num_generate)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_text\u001b[39m(model, start_string, num_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Преобразуем начальную строку в индексы\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     input_eval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43m[\u001b[49m\u001b[43mchar_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart_string\u001b[49m\u001b[43m]\u001b[49m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Генерация текста\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     text_generated \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[54], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_text\u001b[39m(model, start_string, num_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Преобразуем начальную строку в индексы\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     input_eval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[43mchar_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m start_string]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Генерация текста\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     text_generated \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyError\u001b[0m: 'H'"
     ]
    }
   ],
   "source": [
    "# Шаг 5: Генерация текста\n",
    "def generate_text(model, start_string, num_generate=1000):\n",
    "    # Преобразуем начальную строку в индексы\n",
    "    input_eval = torch.tensor([[char_to_index[char] for char in start_string]], dtype=torch.long)\n",
    "\n",
    "    # Генерация текста\n",
    "    text_generated = []\n",
    "    hidden = model.init_hidden(1)\n",
    "    for _ in range(num_generate):\n",
    "        output, hidden = model(input_eval, hidden)\n",
    "        # Получаем следующий символ\n",
    "        predicted_id = torch.argmax(output, dim=2)[-1].item()\n",
    "        text_generated.append(index_to_char[predicted_id])\n",
    "        # Обновляем вход\n",
    "        input_eval = torch.tensor([[predicted_id]], dtype=torch.long)\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Пример генерации текста\n",
    "print(generate_text(model, start_string=\"Homer: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
