{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим необходимые библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Определяем устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для шифрования текста\n",
    "def caesar_cipher(text, shift):\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in CHAR_TO_INDEX:\n",
    "            idx = (CHAR_TO_INDEX[char] + shift) % VOCAB_SIZE\n",
    "            result.append(INDEX_TO_CHAR[idx])\n",
    "        else:\n",
    "            result.append(char)  # если символ не в алфавите, оставляем как есть\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "def generate_data(num_samples):\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        # Генерируем случайную фразу длиной от 10 до MAX_LEN символов\n",
    "        length = random.randint(10, MAX_LEN)\n",
    "        text = \"\".join(random.choices(ALPHABET, k=length))\n",
    "        shift = random.randint(1, 10)  # случайный сдвиг от 1 до 10\n",
    "        encrypted_text = caesar_cipher(text, shift)  # шифруем текст\n",
    "        data.append((encrypted_text, text, shift))  # сохраняем зашифрованный текст, исходный текст и сдвиг\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Построим нейронную сеть\n",
    "\n",
    "1. Разбьем данные на токены (у нас символы)\n",
    "2. Закодируем числами\n",
    "3. Превратим в эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "ALPHABET = string.ascii_lowercase + \" \"  # алфавит (буквы + пробел)\n",
    "CHAR_TO_INDEX = {char: idx for idx, char in enumerate(ALPHABET)}  # символ → индекс\n",
    "INDEX_TO_CHAR = {idx: char for char, idx in CHAR_TO_INDEX.items()}  # индекс → символ\n",
    "VOCAB_SIZE = len(ALPHABET)  # размер словаря\n",
    "MAX_LEN = 50  # максимальная длина фразы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование текста в тензор\n",
    "def text_to_tensor(text, max_len):\n",
    "    tensor = torch.zeros(max_len, dtype=torch.long).to(device)  # перемещаем на устройство\n",
    "    for i, char in enumerate(text[:max_len]):\n",
    "        tensor[i] = CHAR_TO_INDEX.get(char, CHAR_TO_INDEX[\" \"])  # неизвестные символы → пробел\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256  # увеличенный размер скрытого состояния\n",
    "EPOCHS = 30  # увеличенное количество эпох\n",
    "BATCH_SIZE = 128  # увеличенный размер батча\n",
    "LEARNING_RATE = 0.001  # уменьшенный learning rate\n",
    "DROPOUT = 0.2  # добавлен Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация сети с RNN\n",
    "3 слоя:\n",
    "1. Embeding (30)\n",
    "2. RNN (hidden_dim=128)\n",
    "3. Полносвязный слой для предсказания буквы (28, то есть размер словаря)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейронная сеть с учетом сдвига\n",
    "class CaesarDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout):\n",
    "        super(CaesarDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.shift_embedding = nn.Embedding(11, hidden_size)  # сдвиги от 0 до 10\n",
    "        self.rnn = nn.GRU(hidden_size * 2, hidden_size, batch_first=True, num_layers=2, dropout=dropout)  # вход: символы + сдвиг\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, shift):\n",
    "        char_emb = self.embedding(x)  # эмбеддинги символов\n",
    "        shift_emb = self.shift_embedding(shift).unsqueeze(1).expand(-1, x.size(1), -1)  # эмбеддинги сдвига\n",
    "        combined = torch.cat((char_emb, shift_emb), dim=2)  # объединяем символы и сдвиг\n",
    "        x, _ = self.rnn(combined)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "data = generate_data(20000)  # генерируем 20,000 примеров\n",
    "encrypted_texts, original_texts, shifts = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем тексты в тензоры\n",
    "X = torch.stack([text_to_tensor(text, MAX_LEN) for text in encrypted_texts]).to(device)  # перемещаем на устройство\n",
    "Y = torch.stack([text_to_tensor(text, MAX_LEN) for text in original_texts]).to(device)  # перемещаем на устройство\n",
    "shifts = torch.tensor(shifts, dtype=torch.long).to(device)  # преобразуем сдвиги в тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель, функцию потерь и оптимизатор\n",
    "model = CaesarDecoder(VOCAB_SIZE, HIDDEN_SIZE, DROPOUT).to(device)  # перемещаем модель на устройство\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # перемещаем функцию потерь на устройство\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # используем Adam\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # добавляем scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0  # для подсчета правильных предсказаний\n",
    "    total_samples = 0  # для подсчета общего числа символов\n",
    "\n",
    "    for i in range(0, len(X), BATCH_SIZE):\n",
    "        # Берем батч\n",
    "        X_batch = X[i:i + BATCH_SIZE]\n",
    "        Y_batch = Y[i:i + BATCH_SIZE]\n",
    "        shift_batch = shifts[i:i + BATCH_SIZE]  # сдвиги для батча\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(X_batch, shift_batch)  # передаем сдвиг в модель\n",
    "        outputs = outputs.view(-1, VOCAB_SIZE)\n",
    "        Y_batch = Y_batch.view(-1)\n",
    "\n",
    "        # Вычисляем потери\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Обратный проход и обновление весов\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Вычисляем accuracy\n",
    "        _, predicted = torch.max(outputs, dim=1)  # получаем предсказанные символы\n",
    "        total_correct += (predicted == Y_batch).sum().item()  # считаем правильные предсказания\n",
    "        total_samples += Y_batch.size(0)  # общее количество символов в батче\n",
    "\n",
    "    # Обновляем learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Выводим статистику\n",
    "    accuracy = total_correct / total_samples  # вычисляем accuracy\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / (len(X) // BATCH_SIZE):.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка качества\n",
    "def decode_text(model, encrypted_text, shift):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Ограничиваем длину входного текста\n",
    "        input_length = len(encrypted_text)\n",
    "        tensor = text_to_tensor(encrypted_text, input_length).unsqueeze(0).to(device)  # перемещаем на устройство\n",
    "        shift_tensor = torch.tensor([shift], dtype=torch.long).to(device)  # преобразуем сдвиг в тензор\n",
    "        output = model(tensor, shift_tensor)\n",
    "        _, indices = torch.max(output, dim=2)\n",
    "        decoded_text = \"\".join([INDEX_TO_CHAR[idx.item()] for idx in indices[0][:input_length]])  # обрезаем до длины исходного текста\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "texts = [\n",
    "    \"hello world\", \"neural network\", \"caesar cipher\", \"deep learning\",\n",
    "    \"machine learning\", \"artificial intelligence\", \"data science\",\n",
    "    \"python programming\", \"pytorch framework\", \"encryption algorithms\"\n",
    "]\n",
    "\n",
    "print(\"Testing the model on multiple examples:\")\n",
    "for text in texts:\n",
    "    shift = random.randint(1, 10)  # случайный сдвиг от 1 до 10\n",
    "    encrypted_text = caesar_cipher(text, shift)\n",
    "    decoded_text = decode_text(model, encrypted_text, shift)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Shift: {shift}\")\n",
    "    print(f\"Encrypted: {encrypted_text}\")\n",
    "    print(f\"Decoded: {decoded_text}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
