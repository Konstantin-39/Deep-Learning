{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "# Параметры\n",
    "ALPHABET = string.ascii_lowercase + \" \"  # алфавит (буквы + пробел)\n",
    "CHAR_TO_INDEX = {char: idx for idx, char in enumerate(ALPHABET)}  # символ → индекс\n",
    "INDEX_TO_CHAR = {idx: char for char, idx in CHAR_TO_INDEX.items()}  # индекс → символ\n",
    "VOCAB_SIZE = len(ALPHABET)  # размер словаря\n",
    "MAX_LEN = 50  # максимальная длина фразы\n",
    "HIDDEN_SIZE = 256  # размер скрытого состояния\n",
    "EPOCHS = 30  # количество эпох\n",
    "BATCH_SIZE = 128  # размер батча\n",
    "LEARNING_RATE = 0.001  # learning rate\n",
    "DROPOUT = 0.2  # Dropout\n",
    "\n",
    "# Определяем устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('simpsons_script_lines.csv')\n",
    "phrases = df['normalized_text'].dropna().tolist()  # используем столбец normalized_text\n",
    "\n",
    "# Преобразуем тексты в список символов\n",
    "text = [[char for char in phrase] for phrase in phrases if isinstance(phrase, str)]\n",
    "\n",
    "# Создаем тензор для хранения индексов символов\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=torch.long).to(device)  # перемещаем на устройство\n",
    "\n",
    "# Заполняем тензор индексами символов\n",
    "for i, phrase in enumerate(text):\n",
    "    for j, char in enumerate(phrase):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(char, CHAR_TO_INDEX[\" \"])  # неизвестные символы → пробел\n",
    "\n",
    "# Кастомная RNN-ячейка на основе полносвязных слоев\n",
    "class CustomRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)  # вход -> скрытое состояние\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)  # скрытое состояние -> скрытое состояние\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hidden = torch.tanh(self.i2h(x) + self.h2h(hidden))  # обновляем скрытое состояние\n",
    "        return hidden\n",
    "\n",
    "# Нейронная сеть с кастомной RNN-ячейкой\n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn_cell = CustomRNNCell(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(x.size(0), self.hidden_size).to(device)  # инициализация скрытого состояния\n",
    "        x = self.embedding(x)\n",
    "        outputs = []\n",
    "        for i in range(x.size(1)):  # проходим по каждому символу в последовательности\n",
    "            hidden = self.rnn_cell(x[:, i, :], hidden)  # обновляем скрытое состояние\n",
    "            outputs.append(hidden)\n",
    "        outputs = torch.stack(outputs, dim=1)  # объединяем выходы\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs, hidden\n",
    "\n",
    "# Подготовка данных\n",
    "X = X.to(device)\n",
    "Y = torch.roll(X, shifts=-1, dims=1).to(device)  # сдвигаем X на 1 влево для получения целевых символов\n",
    "\n",
    "# Создаем модель, функцию потерь и оптимизатор\n",
    "model = TextGenerator(VOCAB_SIZE, HIDDEN_SIZE, DROPOUT).to(device)  # перемещаем модель на устройство\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # перемещаем функцию потерь на устройство\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # используем Adam\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # добавляем scheduler\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0  # для подсчета правильных предсказаний\n",
    "    total_samples = 0  # для подсчета общего числа символов\n",
    "\n",
    "    for i in range(0, len(X), BATCH_SIZE):\n",
    "        # Берем батч\n",
    "        X_batch = X[i:i + BATCH_SIZE]\n",
    "        Y_batch = Y[i:i + BATCH_SIZE]\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs, _ = model(X_batch)\n",
    "        outputs = outputs.view(-1, VOCAB_SIZE)\n",
    "        Y_batch = Y_batch.view(-1)\n",
    "\n",
    "        # Вычисляем потери\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Обратный проход и обновление весов\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Вычисляем accuracy\n",
    "        _, predicted = torch.max(outputs, dim=1)  # получаем предсказанные символы\n",
    "        total_correct += (predicted == Y_batch).sum().item()  # считаем правильные предсказания\n",
    "        total_samples += Y_batch.size(0)  # общее количество символов в батче\n",
    "\n",
    "    # Обновляем learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Выводим статистику\n",
    "    accuracy = total_correct / total_samples  # вычисляем accuracy\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / (len(X) // BATCH_SIZE):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Генерация текста\n",
    "def generate_text(model, start_text, max_length=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Преобразуем начальный текст в тензор\n",
    "        input_tensor = text_to_tensor(start_text, len(start_text)).unsqueeze(0).to(device)\n",
    "        hidden = None\n",
    "        generated_text = list(start_text)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            _, predicted = torch.max(output[:, -1, :], dim=1)\n",
    "            next_char = INDEX_TO_CHAR[predicted.item()]\n",
    "            generated_text.append(next_char)\n",
    "            input_tensor = text_to_tensor(next_char, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        return \"\".join(generated_text)\n",
    "\n",
    "# Пример генерации текста\n",
    "start_text = \"homer\"\n",
    "generated_text = generate_text(model, start_text, max_length=100)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
